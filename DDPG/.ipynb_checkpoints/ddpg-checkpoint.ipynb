{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bipedal walker using DDPG\n",
    "### Credits: github/udacity-deep-reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qasim\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = 'BipedalWalker-v3'\n",
    "env = gym.make(env_id)\n",
    "env.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to GPU if available (Udacity env)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0] #state size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.shape[0] #action size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Since action space is continuous, we can implement an Actor-Critic method algorithm known as DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor (policy) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        return torch.tanh(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=256, fc3_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc2.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc3.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Define Noise model. \n",
    "<p>Let's define the noise process model using the standard Ornstein-Uhlenbeck process </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Replay Buffer\n",
    "<p> A DQN operates with a replay buffer that uncorrelates multiple sequences within a trajectory. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Define main DDPG Agent\n",
    "<p> Let's tie everything from part 2 and 3 together to build our DDPG agent. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants:\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=10):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed. default = 10.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Train the agent\n",
    "<p> Now, we are on our last step, and that is to train the actual agent. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size) #define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_global = [] #just in case the kernel gets interrupt before completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=10000, max_t=700):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        scores_global.append(score)#global\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            actor_model_name = 'models/checkpoint_actor' + str(\"%05d\" % (i//100)) + '.pth'\n",
    "            critic_model_name = 'models/checkpoint_critic' + str(\"%05d\" % (i//100)) + '.pth'\n",
    "            torch.save(agent.actor_local.state_dict(), actor_model_name)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_model_name)\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14\tAverage Score: -103.02\tScore: -102.86"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4a8419b80771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-1b96fd6959c6>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-60faf0710e01>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-60faf0710e01>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Compute critic loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mQ_expected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Minimize the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a09ebf31359b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcs1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_global)+1), scores_global)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6. Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('models/checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('models/checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1) #human is slow.\n",
    "state = env.reset()\n",
    "agent.reset()   \n",
    "total_reward = 0\n",
    "ep_len = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done or ep_len > 1e3:\n",
    "        print(ep_len)\n",
    "        break\n",
    "    ep_len += 1\n",
    "print(f\"Reward:{total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
