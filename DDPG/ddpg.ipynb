{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bipedal walker using DDPG\n",
    "### Credits: github/udacity-deep-reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qasim\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = 'BipedalWalker-v3'\n",
    "env = gym.make(env_id)\n",
    "env.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to GPU if available (Udacity env)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0] #state size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.shape[0] #action size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Since action space is continuous, we can implement an Actor-Critic method algorithm known as DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor (policy) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        return torch.tanh(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=256, fc3_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc2.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc3.weight.data.uniform_(-1.5e-3, 1.5e-3)\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Define Noise model. \n",
    "<p>Let's define the noise process model using the standard Ornstein-Uhlenbeck process </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Replay Buffer\n",
    "<p> A DQN operates with a replay buffer that uncorrelates multiple sequences within a trajectory. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Define main DDPG Agent\n",
    "<p> Let's tie everything from part 2 and 3 together to build our DDPG agent. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants:\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=10):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed. default = 10.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Train the agent\n",
    "<p> Now, we are on our last step, and that is to train the actual agent. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size) #define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_global = [] #just in case the kernel gets interrupt before completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=10000, max_t=700):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        scores_global.append(score)#global\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            actor_model_name = 'models/checkpoint_actor' + str(\"%03d\" % (i_episode//100)) + '.pth'\n",
    "            critic_model_name = 'models/checkpoint_critic' + str(\"%03d\" % (i_episode//100)) + '.pth'\n",
    "            torch.save(agent.actor_local.state_dict(), actor_model_name)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_model_name)\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -106.99\tScore: -116.08\n",
      "Episode 200\tAverage Score: -118.86\tScore: -115.07\n",
      "Episode 300\tAverage Score: -110.79\tScore: -118.50\n",
      "Episode 400\tAverage Score: -99.50\tScore: -111.158\n",
      "Episode 500\tAverage Score: -112.54\tScore: -112.77\n",
      "Episode 600\tAverage Score: -103.51\tScore: -59.202\n",
      "Episode 700\tAverage Score: -87.46\tScore: -61.3244\n",
      "Episode 800\tAverage Score: -113.57\tScore: -121.70\n",
      "Episode 900\tAverage Score: -107.26\tScore: -117.49\n",
      "Episode 1000\tAverage Score: -121.97\tScore: -166.50\n",
      "Episode 1100\tAverage Score: -95.91\tScore: -40.9119\n",
      "Episode 1200\tAverage Score: -73.11\tScore: -112.15\n",
      "Episode 1300\tAverage Score: -104.16\tScore: -146.80\n",
      "Episode 1400\tAverage Score: -70.60\tScore: -24.7686\n",
      "Episode 1500\tAverage Score: -45.68\tScore: 42.1214\n",
      "Episode 1600\tAverage Score: -26.93\tScore: -34.189\n",
      "Episode 1700\tAverage Score: -62.25\tScore: -116.99\n",
      "Episode 1800\tAverage Score: -46.24\tScore: -131.00\n",
      "Episode 1900\tAverage Score: -76.01\tScore: -102.01\n",
      "Episode 2000\tAverage Score: -98.36\tScore: -110.16\n",
      "Episode 2100\tAverage Score: -119.78\tScore: -117.19\n",
      "Episode 2200\tAverage Score: -119.07\tScore: -98.904\n",
      "Episode 2300\tAverage Score: -111.49\tScore: -102.07\n",
      "Episode 2400\tAverage Score: -77.16\tScore: -156.132\n",
      "Episode 2500\tAverage Score: -87.77\tScore: -99.902\n",
      "Episode 2600\tAverage Score: -101.06\tScore: -101.77\n",
      "Episode 2700\tAverage Score: -103.22\tScore: -101.29\n",
      "Episode 2800\tAverage Score: -103.45\tScore: -111.36\n",
      "Episode 2900\tAverage Score: -124.21\tScore: -136.95\n",
      "Episode 3000\tAverage Score: -115.41\tScore: -116.82\n",
      "Episode 3100\tAverage Score: -107.64\tScore: -43.870\n",
      "Episode 3200\tAverage Score: -107.62\tScore: -119.76\n",
      "Episode 3300\tAverage Score: -88.69\tScore: -37.7858\n",
      "Episode 3400\tAverage Score: -112.90\tScore: -99.564\n",
      "Episode 3500\tAverage Score: -93.76\tScore: -99.7832\n",
      "Episode 3600\tAverage Score: -95.59\tScore: -100.14\n",
      "Episode 3700\tAverage Score: -100.01\tScore: -99.893\n",
      "Episode 3800\tAverage Score: -100.09\tScore: -106.00\n",
      "Episode 3900\tAverage Score: -100.38\tScore: -108.54\n",
      "Episode 4000\tAverage Score: -110.02\tScore: -127.99\n",
      "Episode 4100\tAverage Score: -55.49\tScore: -29.0955\n",
      "Episode 4200\tAverage Score: -70.21\tScore: -100.09\n",
      "Episode 4300\tAverage Score: -82.06\tScore: -101.78\n",
      "Episode 4400\tAverage Score: -91.84\tScore: -47.089\n",
      "Episode 4500\tAverage Score: -39.48\tScore: -19.809\n",
      "Episode 4600\tAverage Score: -115.89\tScore: -147.99\n",
      "Episode 4700\tAverage Score: -124.27\tScore: -118.53\n",
      "Episode 4772\tAverage Score: -116.77\tScore: -116.11"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4a8419b80771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-a6e5538dd8f0>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-60faf0710e01>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d9101156127d>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;34m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    339\u001b[0m                     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_global[:2000])+1), scores_global[:2000])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6. Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('models/checkpoint_actor016.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('models/checkpoint_critic016.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "Reward:-28.68794959677605\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1) #human is slow.\n",
    "state = env.reset()\n",
    "agent.reset()   \n",
    "total_reward = 0\n",
    "ep_len = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done or ep_len > 1e3:\n",
    "        print(ep_len)\n",
    "        break\n",
    "    ep_len += 1\n",
    "print(f\"Reward:{total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
