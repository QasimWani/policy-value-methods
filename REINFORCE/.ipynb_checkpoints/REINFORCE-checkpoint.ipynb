{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement REINFORCE (Monte Carlo Policy Gradients) to solve LunarLander task in OpenAI-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical #bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(8,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check for bounds\n",
    "env.observation_space.is_bounded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2 #set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"Defines the general policy for an agent following simple NN architecture\"\"\"\n",
    "    def __init__(self, action_size=4, state_size=8, h1=16, h2=8):\n",
    "        \"\"\"Creates the model using a 3 Hidden layer NN\"\"\"\n",
    "        super(Policy, self).__init__()#inherit methods from parent class & override forward f(x)\n",
    "        self.action_space = action_size\n",
    "        self.state_space = state_size\n",
    "        self.fc1 = nn.Linear(in_features=state_size, out_features=h1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=h1, out_features=h2, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=h2, out_features=action_size, bias=True)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs one-pass from state -> action mapping.\n",
    "        @Param:\n",
    "        1. x - input state\n",
    "        @return:\n",
    "        x - action as a set of vector following stochastic measure. softmax output to logits from NN.\n",
    "        \"\"\"\n",
    "        if(type(x) != torch.Tensor):\n",
    "            try:\n",
    "                x = torch.from_numpy(x).float().unsqueeze(0)#convert ndarray to torch.Tensor object\n",
    "            except:\n",
    "                raise TypeError(f\"expected type torch.Tensor. got {type(x)}\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Uses current deterministic policy to determine the set of action to perform\n",
    "        @param:\n",
    "        1. state: input state of env. shape = env.observation_space.shape[0]\n",
    "        @return:\n",
    "        - action: (int) discrete action to take by the agent.\n",
    "        - log_probs: (array_like) log of output from softmax unit. set of log probabilities.\n",
    "        \"\"\"\n",
    "        probs = self.forward(state) #get estimated action following stochastic measure\n",
    "        m = Categorical(probs)#get Bernoulli distribution of action\n",
    "        action = m.sample() #returns the action based on the probability of each based on Benoulli(probs)\n",
    "        return action, m.log_prob(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()#instantiate Policy obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset() #sample random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3]),\n",
       " tensor([[-1.6292, -1.6292, -1.6292, -1.6292]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.act(state) #Test to see if policy works (random weights initially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.Adam(params=policy.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(num_episode=1000, max_tau=1000, gamma=1.0, print_every=10):\n",
    "    \"\"\"\n",
    "    Implements the Reinforce algorithm.\n",
    "    See paper for more details: https://bit.ly/REINFORCE_paper\n",
    "    @param:\n",
    "    1. num_episode: number of epochs to train for.\n",
    "    2. max_tau: length of trajectory, ùùâ.\n",
    "    3. gamma: discounted return, Œ≥.\n",
    "    4. print_every: pprint details after very X epochs.\n",
    "    @return:\n",
    "    - scores: (array_like) expected return over epochs.\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)#determines if episode is solved by determining score of the last N episodes\n",
    "    scores = []\n",
    "    for i_episode in range(1, num_episode + 1):\n",
    "        saved_log_probs = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
