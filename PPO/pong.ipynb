{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using PPO (Proximal Policy Optimization) algorithm to converge to optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from parallelEnv import parallelEnv\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(210, 160, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #grid-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings() #6 discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "def preprocess_single_frame(image, bkg_color = np.array([144, 72, 17])):\n",
    "    \"\"\"\n",
    "    Converts an image from RGB channel to B&W channels.\n",
    "    Also performs downscale to 80x80. Performs normalization.\n",
    "    @Param:\n",
    "    1. image: (array_like) input image. shape = (210, 160, 3)\n",
    "    2. bkg_color: (np.array) standard encoding for brown in RGB with alpha = 0.0\n",
    "    @Return:\n",
    "    - img: (array_like) B&W, downscaled, normalized image of shape (80x80)\n",
    "    \"\"\"\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    \"\"\"\n",
    "    convert outputs of parallelEnv to inputs to pytorch neural net\"\"\"\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def collect_trajectories(env, policy, tmax=200, nrand=5):\n",
    "    \"\"\"collect trajectories for an environment\"\"\"\n",
    "    # number of parallel instances\n",
    "    n=1\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    env.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    env.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = env.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = env.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = env.step(action)\n",
    "        fr2, re2, is_done, _ = env.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, \\\n",
    "        action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state shape: (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()#initial state\n",
    "print(\"Initial state shape:\", state.shape)\n",
    "\n",
    "for i in range(20):#play 20 frames\n",
    "    frame, _, _, _ = env.step(np.random.randint(0, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD3CAYAAADmBxSSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdX0lEQVR4nO3de5RcVZ328e9jICCCBEhATYCgRkZwaWTyIshSUURAkYtLHRgVRDQwgiOvrFcBZ0a8oDgjIC4UDcrF4S7KkFFGwCgyjoIkgHIJDCEGE4hJuAmigoHn/ePslkqnOl3dVdVVfXg+a/WqOvvcflWV/GrXPpefbBMREfXynF4HEBERnZfkHhFRQ0nuERE1lOQeEVFDSe4RETWU5B4RUUNJ7m2Q9HVJ/9zpZYfZznRJlrTeEPNvl7R7u/uJiPFNOc99fJE0HfgNsL7t1b2NJiL6VXruoyRpQq9jiIgYSpJ7A0kvl3StpEfK8MZ+DfPOlXSmpCslPQ68sbR9rmGZj0taLul+SR8swycvbVj/c+X57pKWSTpW0sqyzmEN23mbpJslPSppqaQTR/Aalkh6c3l+oqTvSDpf0mOSbpX0MknHl/0ulfSWhnUPk7SwLLtY0hGDtr2u17eBpC9J+q2kFWUY6rkj/QwiojOS3AtJ6wP/CVwNbAl8BLhA0vYNi/09cBKwCfCzQevvDXwMeDPwUuANw+zyBcCmwFTgcOCrkjYr8x4HDgEmAW8D/kHSAaN8aW8H/h3YDLgZuIrqc58KfAb4RsOyK4F9gecDhwGnSdqpxdf3ReBlwMwyfyrwL6OMOSLalOT+jF2AjYGTbT9p+8fA94GDG5a5wvb/2H7a9p8Hrf9u4Bzbt9v+I/DpYfb3F+Aztv9i+0rgD8D2ALavtX1r2c+vgYsY/stiKP9t+6oyPv8dYEp5jX8BLgamS5pU9vsD2/e48lOqL7rXDff6JAn4EPB/bT9k+zHg88BBo4w5ItrU9IyLZ6kXAUttP93Qdi9VD3TA0mHWn9/isgAPDjog+keqLxckvQY4GXgFMBHYgCoxj8aKhud/Ah6w/VTDNGW/j0jaB/gUVQ/8OcBGwK1lmXW9vill2QVVngdAQI5LRPRIeu7PuB/YWlLje7INcF/D9LpOLVoOTGuY3rqNWC4E5gJb294U+DpVsuwaSRsA3wW+BGxlexJwZcN+1/X6HqD6otjR9qTyt6ntjbsZc0QMLcn9GTdQjXV/XNL65Vzxt1MNXbTiUuCwclB2I9obb94EeMj2nyXtTDXW320DvxBWAatLL/4tDfOHfH3l185ZVGP0WwJImipprzGIOyKaSHIvbD8J7AfsQ9UT/RpwiO07W1z/v4CvAD8BFgG/KLOeGEU4HwY+I+kxqiR66Si2MSJlnPwfy74epvpCmdswf7jX94nSfr2kR4EfUY4hRMTYy0VMXSLp5cBtwAZ1vNio7q8vYrxLz72DJB0oaWI5pfGLwH/WKfHV/fVF1EmSe2cdQTVmfQ/wFPAPvQ2n4+r++iJqo2vDMuWil9OpTof7pu2Tu7KjiIhYS1eSe7nvyv8CewLLgBuBg23f0fGdRUTEWro1LLMzsMj24nIWysXA/l3aV0REDNKtK1SnsuYVjMuA1wy1sKR1/nzY+vm50DHas/TRpx6wPaXXcUSMlW4l92ZXU66RwCXNBmYDbLbhc/jU7pt2KZTW7fnaXUe0/DU//8XwC9Xc/I+9reVlZ536gy5Gsm7H/PDhe3u284ge6NawzDLWvDx9GtXl/X9le47tWbZnbTyxq1fWR0Q863Qrud8IzJC0naSJVHcHnDvMOhER0SFdGZaxvVrS0VT3Dp8AnG379m7sKyIi1ta1W/6We5Rf2a3tj4XBY+ojHZN/Nho8rj6SMfmI6JxcoRoRUUNJ7hERNZTkHhG1Uwrdf3CIeSdI+uZYxzTWUmYvIp5VbH++1zGMhfTcI2pCUkc7a53eXoytJPeIPiZpiaTjJd0h6WFJ50jasMzbXdIySZ+Q9DvgnNK+r6RbJD0i6eeSXtnm9j4kaZGkhyTNlfSihu3tKOmaMm+FpBNK+3MkHSfpHkkPSrpU0uZl3oaSzi/tj0i6UdJWZd77JS2W9Jik30h6T8O+PiBpYYn7KknbNszbU9Kdkn4v6QzWUXNY0omSzi/Pp0uypMMkLS3bPlLS/5H06xLfGQ3rvkTSj0vsD0i6QNKkhvk7Sbq5xP8dSZdI+lzD/CE/m05Lco/of+8B9gJeArwM+KeGeS8ANge2BWZL2gk4m+re+1sA3wDmqiqAPprtvQn4AvBu4IXAvZS6wpI2oSqn+EPgRcBLgXllO/8IHAC8ocx7GPhqmXcosCnVVexbAEcCf5L0PKpSjvvY3gR4LXBL2dcBwAnAO4ApwH8DF5V5k6mKu/8TMJmq3sBuw7+ta3gNMAP4O+DLwCeBNwM7Au+W9IaynMr78SLg5eU1nFjimAhcDpxb3sOLgAMHdtDiZ9MxSe4R/e8M20ttPwScBBzcMO9p4FO2n7D9J+BDwDds32D7KdvnUdW53WWU23sP1UWIN9l+Ajge2FXSdGBf4He2T7H9Z9uP2b6hbOcI4JO2l5X1TgTeWYZ6/kKV3F5aYlxg+9GG/b9C0nNtL2+4+PEI4Au2F5bqX58HZpbe+1uBO2xfZvsvVMn5dyN8jz9bXsPVwOPARbZX2r6P6ovk1QC2F9m+prw/q4BTqb7AKO/xesBXbP/F9veAXzbso5XPpmOS3CP6X+MdVu+l6jUOWGX7zw3T2wLHlp/9j0h6hKp32bjOSLb3orIMALb/ADxIdefXral6yc1sC1zeEMNCqupdWwH/TnX1+sWS7pf0r5LWt/04Vc/5SGC5pB9I+puG7Z3esL2HqHrRU0uMf31NropUNL7GVqxoeP6nJtMbA0jaUtLFku5TVQj+fKpfC5Q47vOaRTIa42jls+mYJPeI/td4E75tWPMmfINvl70UOMn2pIa/jWxfNMrt3U+VlAAoQydbAPeVfb1kiJiXUg2vNMaxoe37Sq/207Z3oBp62Rc4BMD2Vbb3pBoCuhM4q2F7Rwza3nNt/xxY3viaJGnQa+ykL1C9R6+0/XzgvTwzvr8cmFr2P6AxjlY+m47J0fB1yO0GRi63G+iKoyR9H/gj1bjzJetY9iyqHvOPqIYENgJ2B66z/dgotnchVQ/7Qqre9+eBG2wvkfQgcKqkY4AzgYnADmVo5uvASZIOtX2vpCnAa21fIemNwAPAHcCjVMM0T5WDqq+hGrf/E/AHqt4+ZXuflXSL7dslbQq8xfZ3gB8AZ0h6B9UNCo+iOnbQDZsAvwcekTQV+H8N835R4j1a0pnA26gKF11b5rfy2XRMeu4R/e9C4Gpgcfn73FAL2p5PNbZ7BtVBzEXA+9vY3jzgn6kOWC6n6qkfVOY9RlVK8+1UY9x3A28sq55OlWivlvQYcD3PFOx5AXAZVWJfCPyUanjjOcCxVL8WHqIay/5w2dflwBepvmgeBW4D9inzHgDeBZxMNWQ0A/ifoV5Tmz4N7ESV4H8AfG9gRqk69w7gcOARql7996nG1Vv9bDqmawWyR2KbTdfzsa99fq/DSLGOURhHxToW2J7VswBGSdIS4IO2f9SP24t1k3QD8HXb54z1vtNzj4joEElvkPQCSetJOhR4JdWpomNu1GPukrYGvk31E+tpYI7t0yWdSPXTY1VZ9IRy+9++l574yPWyNx7Rh7YHLqU6u+Ye4J22l/cikHYOqK4GjrV9U7mYYYGka8q802x/qf3wIvqLpL2pxpMnAN+0fXI392d7ej9vL9Zkew4wp9dxQBvDMuUCg5vK88eoDoxM7VRgEf1G0gSqqyz3AXYADpa0Q2+jimiuI6dClqvVXg3cQHXZ79GSDgHmU/XuH17X+ptv9wree/68dS0S0ZZjJk8efqHh7Qwssr0YQNLFwP5Up/RF9JW2k7ukjalOkzrG9qPl/M7PUp3o/1ngFOADTdabDcwGmDZtWrthRIyFqax5xeEynjm9r6nJkyd7+vTp3YwpnsWWLFnCAw880PQmaW0ld0nrUyX2C8p9FLC9omH+WVTnea6lcWxq5syZvT8fM2J4zf4TrfVvt7Hjss022zB//vxuxxXPUrNmDX1276jH3Msltt8CFto+taH9hQ2LHUh1sUFEHSxjzcvJp7HmpftA1XGxPcv2rClTpoxZcBGN2um57wa8D7hV0i2l7QSqg0wzqXo0S6ju5hZRBzcCMyRtR3VvlYOAv+9tSBHNjTq52/4ZzX+mjotz2iNGyvZqSUdT3dFwAtWtcG8fZrWInsiNwyJGoFyQlw5M9L3cfiAiooaS3CMiaqgvhmUe+s1tnP/eGb0OIyKiNtJzj4iooST3iIgaSnKPiKihJPeIiBpKco+IqKEk94iIGkpyj4iooST3iIgaSnKPiKihJPeIiBpKco+IqKFO1FBdAjwGPAWstj1L0ubAJcB0qoId7x6uSHZERHROp3rub7Q90/ZAQb/jgHm2ZwDzynRERIyRbg3L7A+cV56fBxzQpf1EREQTnUjuBq6WtKBUfQfYyvZygPK4ZQf2ExERLerE/dx3s32/pC2BayTd2cpK5YtgNsBmG+a4bkREJ7WdVW3fXx5XApcDOwMrJL0QoDyubLLeHNuzbM/aeGKzOtsRETFabSV3Sc+TtMnAc+AtwG3AXODQstihwBXt7CciIkam3WGZrYDLJQ1s60LbP5R0I3CppMOB3wLvanM/ERExAm0ld9uLgVc1aX8Q2KOdbUdExOjlSGZERA0luUdE1FCSe0REDSW5R0TUUJJ7REQNJblHRNRQknvEIJK2lvQTSQsl3S7po6V9c0nXSLq7PG7W61gjhpLkHrG21cCxtl8O7AIcJWkHcivrGEeS3CMGsb3c9k3l+WPAQmAquZV1jCNJ7hHrIGk68GrgBnIr6xhHktwjhiBpY+C7wDG2Hx3BerMlzZc0f9WqVd0LMGIdktwjmpC0PlViv8D290rzsLeyhjVvZz1lypSxCThikCT3iEFU3eb0W8BC26c2zMqtrGPc6EQlpoi62Q14H3CrpFtK2wnAyeRW1jFOJLlHDGL7Z8BQ5cFyK+sYF0ad3CVtD1zS0PRi4F+AScCHgIEjSSfYvnLUEUZExIiNOrnbvguYCSBpAnAfVQ3Vw4DTbH+pIxFGRMSIdeqA6h7APbbv7dD2IiKiDZ1K7gcBFzVMHy3p15LOzv03IiLGXtvJXdJEYD/gO6XpTOAlVEM2y4FThljvrxd6/OFJtxtGREQ06ETPfR/gJtsrAGyvsP2U7aeBs4Cdm63UeKHHxhOHOjEhIiJGoxPJ/WAahmQGruArDgRu68A+IiJiBNo6z13SRsCewBENzf8qaSZgYMmgeRERMQbaSu62/whsMajtfW1FFBERbcu9ZSIiaijJPSKihpLcIyJqKMk9IqKGktwjImooyT0iooaS3CMiaijJPSKihpLcIyJqKMk9IqKGktwjImooyT0iooaS3CMiaijJPSKihpLcIyJqqKXkXgpdr5R0W0Pb5pKukXR3edystEvSVyQtKkWyd+pW8BER0VyrPfdzgb0HtR0HzLM9A5hXpqGqqTqj/M2mKpgdERFjqKXkbvs64KFBzfsD55Xn5wEHNLR/25XrgUmD6qpGRESXtTPmvpXt5QDlccvSPhVY2rDcstIWERFjpBsHVNWkzWstJM2WNF/S/D88udbsiIhoQzvJfcXAcEt5XFnalwFbNyw3Dbh/8Mq259ieZXvWxhObfR9E9JakCZJulvT9Mr2dpBvKSQSXSJrY6xgjhtJOcp8LHFqeHwpc0dB+SDlrZhfg9wPDNxHjzEeBhQ3TXwROKycRPAwc3pOoIlrQ6qmQFwG/ALaXtEzS4cDJwJ6S7gb2LNMAVwKLgUXAWcCHOx51RJdJmga8DfhmmRbwJuCyskjjSQQRfWe9VhayffAQs/ZosqyBo9oJKqIPfBn4OLBJmd4CeMT26jKdEwWir+UK1YhBJO0LrLS9oLG5yaJNzwRoPFlg1apVXYkxYjhJ7hFr2w3YT9IS4GKq4ZgvU12zMfBrt+mJArDmyQJTpkwZi3gj1pLkHjGI7eNtT7M9HTgI+LHt9wA/Ad5ZFms8iSCi7yS5R7TuE8DHJC2iGoP/Vo/jiRhSSwdUI56tbF8LXFueLwZ27mU8Ea1Kzz0iooaS3CMiaijJPSKihpLcIyJqKMk9IqKGktwjImooyT0iooaS3CMiaijJPSKihpLcIyJqaNjkLulsSSsl3dbQ9m+S7pT0a0mXS5pU2qdL+pOkW8rf17sZfERENNdKz/1cYO9BbdcAr7D9SuB/geMb5t1je2b5O7IzYUZExEgMm9xtXwc8NKjt6oaKNNdT3ds6IiL6RCfG3D8A/FfD9HalYvxPJb1uqJUaq9X84cmmBW0iImKU2rrlr6RPAquBC0rTcmAb2w9K+lvgPyTtaPvRwevangPMAdhm0/WS3SMiOmjUyV3SocC+wB6lKDa2nwCeKM8XSLoHeBkwvwOxRjyrXHXVVWtM77XXXj2KpDmpKitb/vtHnxnVsIykvamq0uxn+48N7VMkTSjPXwzMABZ3ItCIiGjdsD13SRcBuwOTJS0DPkV1dswGwDXl2/v6cmbM64HPSFoNPAUcafuhphuOiIiuGTa52z64SXPT2pG2vwt8t92gIqL/ZTimv+UK1YiIGkpyj4iooST3iIgaSnKPiKihti5i6gd7vnbXNaav+fkvehRJRET/SM+9y957/t29DiEinoWS3CMiaijJPSKihpLcu+z8987odQgR8SyU5B4RUUNJ7hFNSJok6bJSTnKhpF0lbS7pGkl3l8fNeh1nxFDG/amQEV1yOvBD2++UNBHYCDgBmGf7ZEnHAcdR3R21K/rtFr8xvqTnHjGIpOdT3eH0WwC2n7T9CLA/cF5Z7DzggN5EGDG8JPeItb0YWAWcU0pGflPS84CtbC8HKI9b9jLIiHUZNrlLOlvSSkm3NbSdKOk+SbeUv7c2zDte0iJJd0nK78oYj9YDdgLOtP1q4HGqIZiWNNYHXrVqVbdijFinVnru5wJ7N2k/zfbM8nclgKQdgIOAHcs6XxuozBQxjiwDltm+oUxfRpXsV0h6IUB5XNlsZdtzbM+yPWvKlCljEnDEYMMmd9vXAa1WU9ofuNj2E7Z/AywCdm4jvogxZ/t3wFJJ25emPYA7gLnAoaXtUOCKHoQX0ZJ2zpY5WtIhVMWvj7X9MDAVuL5hmWWlLWK8+QhwQTlTZjFwGFVn6FJJhwO/Bd7Vw/gi1mm0yf1M4LOAy+MpwAcANVm2aS0uSbOB2QCbbZjjutFfbN8CzGoya4+xjiViNEaV3G2vGHgu6Szg+2VyGbB1w6LTgPuH2MYcYA7ANpuuN+pijLnFb0TE2kbVZR44qFQcCAycSTMXOEjSBpK2A2YAv2wvxIiIGKlhe+6SLgJ2ByZLWgZ8Cthd0kyqIZclwBEAtm+XdCnVwafVwFG2n+pO6BERMZRhk7vtg5s0f2sdy58EnNROUBER0Z4cyYyIqKEk94iIGkpyj4iooST3iIgaSnKPiKihJPeIiBpKco+IqKEk94iIGkpyj4iooST3iIgaSnKPiKihJPeIiBpKco+IqKEk94iIGkpyj4iooVaKdZwN7AustP2K0nYJMFAZfhLwiO2ZkqYDC4G7yrzrbR/Z6aAjIsazX/3qV2tMv+pVr+r4PlqpoXoucAbw7YEG23838FzSKcDvG5a/x/bMTgUYEREj10olputKj3wtkgS8G3hTZ8OKiIh2tDvm/jpghe27G9q2k3SzpJ9Kel2b24+IiFFoZVhmXQ4GLmqYXg5sY/tBSX8L/IekHW0/OnhFSbOB2QCbbZjjuhERnTTqrCppPeAdwCUDbbafsP1geb4AuAd4WbP1bc+xPcv2rI0narRhREREE+10md8M3Gl72UCDpCmSJpTnLwZmAIvbCzEiIkZq2OQu6SLgF8D2kpZJOrzMOog1h2QAXg/8WtKvgMuAI20/1MmAIyJieK2cLXPwEO3vb9L2XeC77YcVAfM/9rY1pmed+oMeRRIx/uRIZkREDSW5R0TUUJJ7REQNJblHRNRQuxcxRcQ6LFiw4AFJjwMP9DqWJiaTuEaiH+PadqgZSe4RXWR7iqT5tmf1OpbBEtfI9GtcQ8mwTEREDSW5R0TUUJJ7RPfN6XUAQ0hcI9OvcTWV5B7RZbb7MikkrpHp17iGkuQeEVFDSe4RXSJpb0l3SVok6bgexrG1pJ9IWijpdkkfLe2bS7pG0t3lcbMexTehFPj5fpneTtINJa5LJE3sQUyTJF0m6c7yvu3aL+9Xq5LcI7qg3Pr6q8A+wA7AwZJ26FE4q4Fjbb8c2AU4qsRyHDDP9gxgXpnuhY8CCxumvwicVuJ6GDi86VrddTrwQ9t/A7yqxNcv71dLZLvXMTBz5kzPmzev12FEjU2ePHnBWJ6jLGlX4ETbe5Xp4wFsf2GsYhiKpCuoit6fAexue7mkFwLX2t5+jGOZBpwHnAR8DHg7sAp4ge3Vg9/HMYrp+cCvgBe7IUFKuosev18jkZ57RHdMBZY2TC8rbT1Vit2/GrgB2Mr2coDyuGUPQvoy8HHg6TK9BfCI7dVluhfv24upvmDOKcNF35T0PPrj/WpZK8U6RjRep8pXyjjjryXt1O0XEdGHmtWO7OnPZEkbU9VbOKZZXeMexLMvsLKU5Pxrc5NFx/p9Ww/YCTjT9quBx+nzIZhmWum5j3S8bh+q8nozqApgn9nxqCP63zJg64bpacD9PYoFSetTJfYLbH+vNK8owwuUx5VjHNZuwH6SlgAXA2+i6slPKjWaoTfv2zJgme0byvRlVMm+1+/XiAyb3G0vt31Tef4Y1YGFqcD+VGNllMcDyvP9gW+7cj3VB/XCjkce0d9uBGaUMz8mUpWlnNuLQCQJ+Baw0PapDbPmAoeW54cCV4xlXLaPtz3N9nSq9+fHtt8D/AR4Zw/j+h2wVNLAePoewB30+P0aqRHdOGxd43WSBsafhhprXN5usBHjRTkYeDRwFTABONv27T0KZzfgfcCtkm4pbScAJwOXlrrIvwXe1aP4BvsEcLGkzwE3U30xjbWPABeUL+bFwGFUneF+fL+aajm5Dx6vqzoDzRdt0rbWmJmk2VTDNkybNq3VMCLGDdtXAlf2QRw/o/n/S6h6pT1n+1rg2vJ8MbBzj+O5BWh2dlVfvF+taOlsmRGO17U01mh7ju1ZtmdtscUWo40/IiKaaOVsmZGO180FDilnzewC/H5g+CYiIsZGK8MyIx2vuxJ4K7AI+CPVWFVERIyhYZP7SMfryhVdR7UZV0REtCFXqEZE1FCSe0REDSW5R0TUUJJ7REQN9cUtfyWtoro5zwO9jmWUJjN+Y4fxHX+rsW9re0q3g4noF32R3AEkzR/L+2130niOHcZ3/OM59ohuyrBMREQNJblHRNRQPyX3Ob0OoA3jOXYY3/GP59gjuqZvxtwjIqJz+qnnHhERHdLz5C5pb0l3lZqr46JOoaQlkm6VdIuk+aWtaU3ZfiDpbEkrJd3W0DYuauAOEfuJku4r7/8tkt7aMO/4EvtdkvbqTdQRvdfT5C5pAvBVqrqrOwAHl/qs48Ebbc9sOA1vqJqy/eBcYO9BbeOlBu65rB07wGnl/Z9ZimJQ/u0cBOxY1vla+TcW8azT6577zsAi24ttP0lVJHf/Hsc0WkPVlO0529cBDw1qHhc1cIeIfSj7AxfbfsL2b6huO93Tij4RvdLr5D5UvdV+Z+BqSQtKuUAYVFMW2HLItfvDUPGOl8/k6DJsdHbDENh4iT2i63qd3Fuqt9qHdrO9E9UQxlGSXt/rgDpoPHwmZwIvAWZSFV4/pbSPh9gjxkSvk3tL9Vb7je37y+NK4HKqn/5D1ZTtV23VwO0l2ytsP2X7aeAsnhl66fvYI8ZKr5P7jcAMSdtJmkh1MGxuj2NaJ0nPk7TJwHPgLcBtDF1Ttl+N2xq4g44BHEj1/kMV+0GSNpC0HdVB4V+OdXwR/aCVGqpdY3u1pKOBq4AJwNm2b+9lTC3YCri8qhvOesCFtn8o6Uaa15TtOUkXAbsDkyUtAz7FOKmBO0Tsu0uaSTXksgQ4AsD27ZIuBe4AVgNH2X6qF3FH9FquUI2IqKFeD8tEREQXJLlHRNRQkntERA0luUdE1FCSe0REDSW5R0TUUJJ7REQNJblHRNTQ/wengYvQxK61BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single_frame(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        ## CNN architecture\n",
    "        #input_channel = 2 stacked frames, i.e. (80x80x2)\n",
    "        # outputsize = (inputsize - kernel_size(aka filter_size) + stride)/stride\n",
    "        # 80x80x2 --> 38x38x4 (Conv1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 --> 9x9x16 (Conv2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=4, stride=4, bias=False)\n",
    "        # TODO: pooling layer\n",
    "        \n",
    "        # FC: 9x9x16 -> 9*9*16 = 1296\n",
    "        self.size = 9*9*16 #1296\n",
    "        \n",
    "        #FC layers\n",
    "        self.fc1 = nn.Linear(in_features=self.size, out_features=256)#standard model\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1) #1 action, left or right\n",
    "        \n",
    "        # define sigmoid for output layer (classification problem, essentially)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs feed forward one-pass convolution for a frame based on the following CNN architecture.\n",
    "        80x80x2 --> 38x38x4 (Conv)\n",
    "        38x38x4 --> 9x9x16  (Conv)\n",
    "        1296 --> 256        (FC)\n",
    "        256 --> 1           (FC)\n",
    "        sigmoid(1)          (output)\n",
    "        ----------\n",
    "        @Param:\n",
    "        1. x: 2 stacked frames of shape: (80x80x2)\n",
    "        @Return:\n",
    "        - out: probability of action (stochastic)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size) #flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(x)) #stochastic\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(\n",
      "  (conv1): Conv2d(2, 4, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
      "  (conv2): Conv2d(4, 16, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "  (fc1): Linear(in_features=1296, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy) #model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4) #Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define actions\n",
    "RIGHT = 4\n",
    "LEFT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Clipped Surrogate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS\n",
    "def states_to_prob(policy, states):\n",
    "    \"\"\"\n",
    "    Convert states to probability, passing through the policy.\n",
    "    @Param:\n",
    "    1. policy: current policy π.\n",
    "    2. states: states pulled from trajectory.\n",
    "    @return:\n",
    "    probabilities of states occurring.\n",
    "    \"\"\"\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Surrogate Function](https://miro.medium.com/max/1400/1*9pJbEtLftES-kgKeonxUYQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![importance sampling ratio](https://i.stack.imgur.com/bCAEy.png)\n",
    "<br>\n",
    "![formula for clipped surrogate function](https://i.stack.imgur.com/zt9mz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "def clipped_surrogate(policy, old_probs, SAR, discount=0.995, epsilon=0.1, beta=0.01):\n",
    "    \"\"\"\n",
    "    Computes the clipped PPO objective function\n",
    "    @Param:\n",
    "    1. policy: current policy\n",
    "    2. old_probs: probability from the old policy.\n",
    "    3. SAR: (tuple) trajectory, (States, Actions, Rewards)\n",
    "    4. discount: discounted return factor.\n",
    "    5. epsilon: baseline clipped range hyperparameter. range b/w 0.1-0.3\n",
    "    6. beta: constant regularizer for entropy calculation.\n",
    "    @Return:\n",
    "    L_clip = clipped surrogate loss function.\n",
    "    Establishes a more conservative reward than regular surrogate f(x)\n",
    "    \"\"\"\n",
    "    states, actions, rewards = SAR #extract from tuple\n",
    "    discounts = discount**np.arange(len(rewards)) # γ^0 + γ^1 + ... + γ^n-1\n",
    "    rewards = np.asarray(rewards)*discounts[:,np.newaxis] #immediate reward in-place calculation\n",
    "    \n",
    "    # convert rewards to future rewards (tackles Credit assignment problem that REINFORCE has)\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "\n",
    "    #perform Z-score based standarization\n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1e-10 #prevent from sigma = 0 when normalizing\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # tensor typecasting\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)#sets conservative rewards\n",
    "\n",
    "    # convert states to probabilities\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs) #calculate theta, theta_prime\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs #importance sampling ratio\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # entropy: regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1e-10 to avoid log(0) which gives nan\n",
    "    log_old_probs_theta = torch.log(old_probs+1e-10)\n",
    "    log_log_probs_theta_prime = torch.log(1.0-old_probs+1e-10)\n",
    "    entropy = -(new_probs*log_old_probs_theta + (1.0-new_probs)*log_log_probs_theta_prime)\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = collect_trajectories(env, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "        SAR = (states, actions, rewards)\n",
    "        L = -clipped_surrogate(policy, old_probs, SAR, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
