{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Asynchronous Advantage Actor Critic (A3C) Policy Gradient Method to solve Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'BreakoutDeterministic-v4'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(210, 160, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:Discrete(4)\n",
      "Meanings:['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actions:{env.action_space}\\nMeanings:{env.unwrapped.get_action_meanings()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #convert to GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "def preprocess_single_frame(image, bkg_color = np.array([144, 72, 17])):\n",
    "    \"\"\"\n",
    "    Converts an image from RGB channel to B&W channels.\n",
    "    Also performs downscale to 80x80. Performs normalization.\n",
    "    @Param:\n",
    "    1. image: (array_like) input image. shape = (210, 160, 3)\n",
    "    2. bkg_color: (np.array) standard encoding for brown in RGB with alpha = 0.0\n",
    "    @Return:\n",
    "    - img: (array_like) B&W, downscaled, normalized image of shape (80x80)\n",
    "    \"\"\"\n",
    "    img = np.mean(image[35:195:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img\n",
    "\n",
    "#Utils\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    \"\"\"\n",
    "    convert outputs of parallelEnv to inputs to pytorch neural net\"\"\"\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(20):#skip 20 frames\n",
    "    frame, _, _, _ = env.step(np.random.randint(0, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot processed and raw image\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single_frame(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![actor critic achitecture](https://www.mdpi.com/energies/energies-09-00725/article_deploy/html/images/energies-09-00725-g001-1024.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE Constants\n",
    "GAMMA = 0.99\n",
    "TAU = 1.0\n",
    "LR = 1e-3\n",
    "MAX_EPISODE_LENGTH = 1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, action_size=4, num_frames=2):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #Define the CNN for Actor & Critic\n",
    "        self.conv1 = nn.Conv2d(num_frames, 32, 3, stride=2, padding=1) #output = 40x40x32\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1) #output = 20x20x32\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1) #output = 10x10x32\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1) #output = 5x5x32\n",
    "        \n",
    "        self.size = 5*5*32 #800\n",
    "\n",
    "        #FC layer\n",
    "        self.lstm = nn.LSTMCell(input_size=self.size, hidden_size=512)#lstm cell to prevent vanishing gradients\n",
    "        \n",
    "        # Define Actor and Critic network\n",
    "        # Critic evaluates the state value function, V(π) using TD estimate.\n",
    "        # Actor evaluates the policy π(a|s) distribution\n",
    "        \n",
    "        self.critic_linear, self.actor_linear = nn.Linear(512, 1), nn.Linear(512, self.action_size)\n",
    "        \n",
    "    def forward(self, x, hx, cx):\n",
    "        \"\"\"\n",
    "        Peforms one-pass for the Conv layers.\n",
    "        @Param:\n",
    "        1. x - shape: (2, 80, 80); 2 stacked frames of 80x80 images\n",
    "        2. hx - hidden state of the RNN. shape: (1x512)\n",
    "        3. cx - confidence state of the RNN. shape: (1x512)\n",
    "        @Return:\n",
    "        1. critic estimated value, V(π)\n",
    "        2. actor policy distribution, π(a|s) as logits\n",
    "        \"\"\"\n",
    "        #4 conv nets without max pool layers, simple Relu activation f(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1,self.size) #flatten\n",
    "        hx, cx = self.lstm(x, (hx, cx)) #dynamic calculation for final confidence & hidden state\n",
    "        value = self.critic_linear(hx) #CRITIC: calculates estimated state value function, V(π)\n",
    "        logits = self.actor_linear(hx) #ACTOR:  calculates policy distribution π(a|s)\n",
    "        \n",
    "        return logits, value, hx, cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCritic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "<p> A critical component to an A3C model is the ability to share parameters across\n",
    "    multiple agents running asynchronously such that they can collectively learn from\n",
    "    each other. This is done by the cross-integration (sharing) of gradients across all processes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedOptimizer(optim.Adam):\n",
    "    \"\"\"Implementation of shared parameter model using Adam optimizer\"\"\"\n",
    "    def __init__(self, params, lr=LR):\n",
    "        super(SharedOptimizer, self).__init__(params)\n",
    "        for group in self.param_groups:\n",
    "            for params in group['params']:\n",
    "                state = self.state[params]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(params.data).share_memory_()\n",
    "                state['exp_avg_sq'] = torch.zeros_like(params.data).share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SharedOptimizer(agent.parameters()) #define optimizer (uses Adam, instead of SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimator \n",
    "<br>\n",
    "<p>\n",
    "    Generalized Advantage Estimator (GAE) helps us pick the best value for N-step boostrapping\n",
    "    by incorporating λ as an added hyper-parameter to tune accordingly that will minimize the\n",
    "    bias-variance tradeoff.\n",
    "    <br>\n",
    "    The derivation can be thought of as <a href=\"https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/#the-generalized-advantage-estimator\">the exponentially-decayed sum of residual terms.</a>\n",
    "</p>\n",
    "<p><strong>See the following <a href=\"https://arxiv.org/pdf/1506.02438.pdf\">derivation</a> for GAE estimator:</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAE Derivation](https://res.cloudinary.com/crammer/image/upload/v1596251771/Screen_Shot_2020-07-31_at_11.15.47_PM_feuhld.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # computes discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(values, log_probs, actions, rewards, gae_lambda=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the policy (actor) and value (critic) loss\n",
    "    @Param:\n",
    "    1. values: (tensor) list of V(s) estimator, critic.\n",
    "    2. log_probs: (tensor) list of π(a|s) softmax output, actor.\n",
    "    3. actions: (tensor) actions taken from rollout of trajectory.\n",
    "    4. rewards: (tensor) rewards based on S,A pairs. true values, used to minimize loss.\n",
    "    5. gae_lambda: (float) [0-1] value of lambda for residual calculation. used in N-step bootstrap.\n",
    "    @Return:\n",
    "    - value_loss: (tensor) critic loss.\n",
    "    - policy_loss: (tensor) actor loss.\n",
    "    \"\"\"\n",
    "    np_values = values.view(-1).data.numpy() #convert torch.tensor to numpy array & flatten/reshape it\n",
    "    \n",
    "    #implement GAE\n",
    "    delta_t = np.asarray(rewards) + GAMMA * (np_values[1:] - np_values[:-1])\n",
    "    \n",
    "    log_probs = log_probs.gather(1, torch.tensor(actions).view(-1,1))\n",
    "    gae = discount(delta_t, GAMMA * TAU)#calculate Generative Advantage Estimate\n",
    "\n",
    "    #calculate policy_loss = -log( π(a|s) ) * ( R - V(s) )\n",
    "    policy_loss = -(log_probs.view(-1) * torch.FloatTensor(gae.copy())).sum()\n",
    "    \n",
    "    # l2 loss over value estimator\n",
    "    rewards[-1] += GAMMA * np_values[-1]\n",
    "    discounted_r = discount(np.asarray(rewards), GAMMA)\n",
    "    discounted_r = torch.tensor(discounted_r.copy(), dtype=torch.float32)\n",
    "    \n",
    "    value_loss = gae_lambda * (discounted_r - values[:-1,0]).pow(2).sum()#set lambda = .5 based on GAE param\n",
    "\n",
    "    entropy_loss = (-log_probs * torch.exp(log_probs)).sum() # entropy = ∑ -log(π(a|s))*e^(log(π(a|s)))\n",
    "    #return policy_loss + 0.5 * value_loss - 0.01 * entropy_loss\n",
    "    return policy_loss + value_loss + entropy_loss #total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def sync_models(model, shared_model):\n",
    "    \"\"\"\n",
    "    Syncs the gradients from local model to shared model as a critical part of the A3C algorithm.\n",
    "    Updates the pointer based reference.\n",
    "    @param:\n",
    "    1. model: local model to sync from.\n",
    "    2. shared_model: global model to sync into.\n",
    "    \"\"\"\n",
    "    for local_param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        if(shared_param.grad is not None):\n",
    "            return\n",
    "        shared_param.grad = local_param.grad #sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_frame(reset=True):\n",
    "    \"\"\"\n",
    "    Returns a 1x2x80x80 tensor by concatenating 2 processed frames.\n",
    "    @Param:\n",
    "    - reset: (boolean, optional) if True, gets the first 2 frames of the env.\n",
    "    @return:\n",
    "    - state: a 1x2x80x80 torch.Tensor\n",
    "    - frame0: first frame of size 80x80 \n",
    "    - frame1: second frame of size 80x80 \n",
    "    \"\"\"\n",
    "    frame0 = env.reset()\n",
    "    frame1,_,_,_ = env.step(1) #fire\n",
    "    state = preprocess_batch([frame0, frame1]) #1x2x80x80 tensor\n",
    "    return state, frame0, frame1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(shared_model, optimizer=None, num_episode=10, num_steps=20):\n",
    "    \"\"\"\n",
    "    Train A3C agent.\n",
    "    @Param:\n",
    "    1. shared_model: instance of ActorCritic class, globally shared model across all parallel agents.\n",
    "    2. optimizer: instance of SharedOptimizer class, default = None (created as local object).\n",
    "    3. num_episode: (int) number of episodes to train for.\n",
    "    4. num_steps: (int) number of forward pass to pass through. default = 20.\n",
    "    \"\"\"\n",
    "    model = ActorCritic() #local model\n",
    "    \n",
    "    if(optimizer is None):#create optimizer\n",
    "        optimizer = SharedOptimizer(shared_model.parameters(), lr=LR)\n",
    "    \n",
    "    model.train()#Set the local model in training mode\n",
    "    \n",
    "    #Extract stacked frames\n",
    "    \n",
    "    state, frame0, frame1 = get_stacked_frame()#1x2x80x80 Tensor\n",
    "    episode_length = 0\n",
    "    done = True\n",
    "    \n",
    "    #return metrics\n",
    "    overall_reward = []\n",
    "\n",
    "    for i in range(1, num_episode+1):\n",
    "        model.load_state_dict(shared_model.state_dict()) #syncs the shared model with the local model\n",
    "            \n",
    "        if done:\n",
    "            cx, hx = torch.zeros(1, 512), torch.zeros(1, 512)\n",
    "        else:\n",
    "            cx = cx.detach()\n",
    "            hx = hx.detach()\n",
    "        \n",
    "        values = []; log_probs = []; rewards = []; actions = []\n",
    "        for step in range(num_steps):\n",
    "            episode_length += 1\n",
    "            logits, value, hx, cx = model(state, hx, cx)\n",
    "            \n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            log_prob = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            action = prob.multinomial(num_samples=1).data[0]\n",
    "            \n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            \n",
    "            #Update frames\n",
    "            frame0 = frame1\n",
    "            frame1 = state\n",
    "            #stack frames to an 1x2x80x80 Tensor\n",
    "            state = preprocess_batch([frame0, frame1])\n",
    "            \n",
    "            #update data\n",
    "            values.append(value.detach())\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "                        \n",
    "            if done or episode_length > MAX_EPISODE_LENGTH:\n",
    "                episode_length = 0\n",
    "                state, frame0, frame1 = get_stacked_frame()#1x2x80x80 Tensor\n",
    "                break\n",
    "                \n",
    "        overall_reward.append(sum(rewards)) #cumulitive reward per episode\n",
    "    \n",
    "    \n",
    "    #Solves broadcasting error\n",
    "    next_value = torch.zeros(1,1) if done else model(state, hx, cx)[1] #return value\n",
    "    values.append(next_value.detach())\n",
    "    \n",
    "    #compute loss\n",
    "    loss = compute_cost(torch.cat(values), torch.cat(log_probs), actions, rewards)\n",
    "    print(f\"cost after {num_episode} episodes: {loss}\")\n",
    "    optimizer.zero_grad()#reset gradient\n",
    "    loss.backward() #perform backprop\n",
    "    \n",
    "    #synchronize shared_model with local_model\n",
    "    sync_models(model, shared_model)\n",
    "    \n",
    "    #Perform single forward step\n",
    "    optimizer.step()\n",
    "    \n",
    "    return overall_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = train(agent, num_episode=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
